
@misc{Mansour,
  author = {Yishay Mansour},
  title = {Computational Learning Theory},
  howpublished = "\url{https://www.tau.ac.il/~mansour/rl-course/scribe6/node3.html}",
  month         = {November},
  year          = {1999}
}

@misc{Zico,
  author = {J. Zico Kolter},
  title = {15-780: Markov Decision Processes},
  howpublished = "\url{http://www.cs.cmu.edu/afs/cs/academic/class/15780-s16/www/slides/mdps.pdf}",
  month         = {February},
  year          = {2016}
}

@misc{lec16,
  author = {Tanmay Gangwani},
  title = {Lecture 16: Value Iteration, Policy Iteration and Policy Gradient},
  howpublished = "\url{https://yuanz.web.illinois.edu/teaching/IE498fa19/lec16.pdf}",
  month         = {October},
  year          = {2019}
}

@misc{L3note,
  author        = {Cathy Wu},
  title         = {6.246 Reinforcement Learning: Foundations and Methods, Lecture 3: Markov Decision Processes},
  month         = {February},
  year          = {2021}
}

@inproceedings{baier,
author = {Baier, Christel and Klein, Joachim and Leuschner, Linda and Parker, David and Wunderlich, Sascha},
year = {2017},
month = {07},
pages = {160-180},
title = {Ensuring the Reliability of Your Model Checker: Interval Iteration for Markov Decision Processes (extended)},
isbn = {978-3-319-63386-2},
doi = {10.1007/978-3-319-63387-9_8}
}

@article{MacQueen,
 ISSN = {0030364X, 15265463},
 URL = {http://www.jstor.org/stable/168468},
 abstract = {In a Markovian decision problem, choice of an action determines an immediate return and the probability of moving to the next state. It is desired to maximize the expected total of discounted future returns. If upper and lower bounds on the optimal expected return are available, a simple test is described that may show that certain actions are suboptimal, permanently eliminating them from further consideration. This test may be incorporated into the dynamic programming routine for solving the decision problem. This was tried on Howard's automobile replacement problem, using the upper and lower bounds described in "A Modified Dynamic Programming Method" (J. Math. Anal. and Appl. 14, April, 1966). The amount of computation required by the dynamic programming routine was reduced, conservatively, by 75 per cent.},
 author = {J. MacQueen},
 journal = {Operations Research},
 number = {3},
 pages = {559--561},
 publisher = {INFORMS},
 title = {A Test for Suboptimal Actions in Markovian Decision Problems},
 volume = {15},
 year = {1967}
}

@misc{silver2015,
		author = {David Silver},
		title = {Lectures on Reinforcement Learning, Lecture 2: Markov Decision Processes},
		howpublished = {\textsc{url:}~\url{https://www.davidsilver.uk/teaching/}},
		year = {2015}
}

@misc{hartmanns2019optimistic,
      title={Optimistic Value Iteration}, 
      author={Arnd Hartmanns and Benjamin Lucien Kaminski},
      year={2019},
      eprint={1910.01100},
      archivePrefix={arXiv},
      primaryClass={cs.LO}
}

@article{HADDAD2018112,
title = {Interval iteration algorithm for MDPs and IMDPs},
journal = {Theoretical Computer Science},
volume = {735},
pages = {111-131},
year = {2018},
note = {Reachability Problems 2014: Special Issue},
issn = {0304-3975},
doi = {https://doi.org/10.1016/j.tcs.2016.12.003},
url = {https://www.sciencedirect.com/science/article/pii/S0304397516307095},
author = {Serge Haddad and Benjamin Monmege},
keywords = {Markov decision processes, Value iteration, Stochastic verification},
abstract = {Markov Decision Processes (MDP) are a widely used model including both non-deterministic and probabilistic choices. Minimal and maximal probabilities to reach a target set of states, with respect to a policy resolving non-determinism, may be computed by several methods including value iteration. This algorithm, easy to implement and efficient in terms of space complexity, iteratively computes the probabilities of paths of increasing length. However, it raises three issues: (1) defining a stopping criterion ensuring a bound on the approximation, (2) analysing the rate of convergence, and (3) specifying an additional procedure to obtain the exact values once a sufficient number of iterations has been performed. The first two issues are still open and, for the third one, an upper bound on the number of iterations has been proposed. Based on a graph analysis and transformation of MDPs, we address these problems. First we introduce an interval iteration algorithm, for which the stopping criterion is straightforward. Then we exhibit its convergence rate. Finally we significantly improve the upper bound on the number of iterations required to get the exact values. We extend our approach to also deal with Interval Markov Decision Processes (IMDP) that can be seen as symbolic representations of MDPs.}
}


@InProceedings{10.1007/978-3-319-11439-2_10,
author="Haddad, Serge
and Monmege, Benjamin",
editor="Ouaknine, Jo{\"e}l
and Potapov, Igor
and Worrell, James",
title="Reachability in MDPs: Refining Convergence of Value Iteration",
booktitle="Reachability Problems",
year="2014",
publisher="Springer International Publishing",
address="Cham",
pages="125--137",
abstract="Markov Decision Processes (MDP) are a widely used model including both non-deterministic and probabilistic choices. Minimal and maximal probabilities to reach a target set of states, with respect to a policy resolving non-determinism, may be computed by several methods including value iteration. This algorithm, easy to implement and efficient in terms of space complexity, consists in iteratively finding the probabilities of paths of increasing length. However, it raises three issues: (1) defining a stopping criterion ensuring a bound on the approximation, (2) analyzing the rate of convergence, and (3) specifying an additional procedure to obtain the exact values once a sufficient number of iterations has been performed. The first two issues are still open and for the third one a ``crude'' upper bound on the number of iterations has been proposed. Based on a graph analysis and transformation of MDPs, we address these problems. First we introduce an interval iteration algorithm, for which the stopping criterion is straightforward. Then we exhibit convergence rate. Finally we significantly improve the bound on the number of iterations required to get the exact values.",
isbn="978-3-319-11439-2"
}

@conference{accelerated,
author={Mohammadsadegh Mohagheghi. and Khayyam Salehi.},
title={Accelerating Interval Iteration for Expected Rewards in Markov Decision Processes},
booktitle={Proceedings of the 15th International Conference on Software Technologies - ICSOFT,},
year={2020},
pages={39-50},
publisher={SciTePress},
organization={INSTICC},
doi={10.5220/0009833700390050},
isbn={978-989-758-443-5},
issn={2184-2833},
}

@inproceedings{haddad,
author = {Haddad, Serge and Monmege, Benjamin},
year = {2014},
month = {09},
pages = {},
title = {Reachability in MDPs: Refining Convergence of Value Iteration},
isbn = {978-3-319-11438-5},
doi = {10.1007/978-3-319-11439-2_10}
}

@misc{brazdil,
      title={Verification of Markov Decision Processes using Learning Algorithms}, 
      author={Tomáš Brázdil and Krishnendu Chatterjee and Martin Chmelík and Vojtěch Forejt and Jan Křetínský and Marta Kwiatkowska and David Parker and Mateusz Ujma},
      year={2015},
      eprint={1402.2967},
      archivePrefix={arXiv},
      primaryClass={cs.LO}
}
